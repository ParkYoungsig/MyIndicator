from __future__ import annotations

import logging
import os
import shutil
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import pandas as pd

try:
    from infra.config import load_yaml_config
except ModuleNotFoundError:  # ì§ì ‘ ì‹¤í–‰ ì‹œ fallback
    import sys as _sys

    _root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
    if _root not in _sys.path:
        _sys.path.append(_root)
    from infra.config import load_yaml_config
try:
    from .cache import read_cache, write_cache
    from .preprocess import preprocess_ohlcv
except ImportError:  # ì§ì ‘ ì‹¤í–‰ ì‹œ fallback
    from infra.data_manager.cache import read_cache, write_cache
    from infra.data_manager.preprocess import preprocess_ohlcv

logger = logging.getLogger(__name__)


def _parse_dt(value: object) -> pd.Timestamp | None:
    try:
        if isinstance(value, pd.Timestamp):
            return value
        ts = pd.to_datetime(str(value), errors="coerce")
        if pd.isna(ts):
            return None
        return pd.Timestamp(ts)
    except Exception:
        return None


def _get_parquet_date_range(
    path: str,
) -> tuple[pd.Timestamp | None, pd.Timestamp | None]:
    """parquetì—ì„œ ë‚ ì§œ ë²”ìœ„ë¥¼ ë¹ ë¥´ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

    - raw í¬ë§·(Date/date ì»¬ëŸ¼)
    - processed í¬ë§·(ì¸ë±ìŠ¤ê°€ dateì¼ ìˆ˜ë„ ìˆìŒ)
    """

    if not path or not os.path.exists(path):
        return None, None

    def _extract(df: pd.DataFrame) -> tuple[pd.Timestamp | None, pd.Timestamp | None]:
        if df is None or df.empty:
            return None, None
        for col in ("date", "Date"):
            if col in df.columns:
                return _parse_dt(df[col].min()), _parse_dt(df[col].max())
        # ë§ˆì§€ë§‰ í´ë°±: ì¸ë±ìŠ¤ê°€ ë‚ ì§œì¸ ê²½ìš°
        if isinstance(df.index, pd.DatetimeIndex):
            return pd.Timestamp(df.index.min()), pd.Timestamp(df.index.max())
        try:
            idx = pd.to_datetime(df.index, errors="coerce")
            idx = idx[~pd.isna(idx)]
            if len(idx) == 0:
                return None, None
            return pd.Timestamp(idx.min()), pd.Timestamp(idx.max())
        except Exception:
            return None, None

    for col in ("Date", "date"):
        try:
            df = pd.read_parquet(path, columns=[col])
            s, e = _extract(df)
            if s is not None or e is not None:
                return s, e
        except Exception:
            pass

    try:
        df = pd.read_parquet(path)
    except Exception:
        return None, None

    return _extract(df)


def load_data_config(name: str = "data") -> Dict[str, Any]:
    return load_yaml_config(name)


def get_krx_universe_codes(
    config: Dict[str, Any], *, refresh_master: bool = False
) -> list[str]:
    """KRX ì¢…ëª©ì½”ë“œë¥¼ íŒŒì¼ ì €ì¥ ì—†ì´ ë©”ëª¨ë¦¬ë¡œë§Œ ë°˜í™˜."""
    cfg = dict(config)
    lists_cfg = dict(cfg.get("lists", {}) or {})
    lists_cfg["refresh_master"] = bool(refresh_master)
    cfg["lists"] = lists_cfg
    sources_cfg = dict(cfg.get("sources", {}) or {})
    # sources_cfg["use_marcap"] = False
    sources_cfg["use_fdr_listing"] = True
    cfg["sources"] = sources_cfg
    downloader = UnifiedDownloader(cfg)
    downloader.refresh_master = bool(refresh_master)
    # downloader.use_marcap = False
    downloader.use_fdr = True
    codes, _ = downloader._get_krx_list()
    return list(dict.fromkeys([str(c).zfill(6) for c in codes if str(c).strip()]))


def _latest_krx_list_csv(base_dir: str) -> str | None:
    try:
        base = Path(base_dir)
        candidates = list(base.rglob("krx_codes_*.csv"))
        if not candidates:
            return None
        candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)
        return str(candidates[0])
    except Exception:
        return None


def _read_csv_codes(
    csv_path: str,
    *,
    code_cols: Iterable[str],
    shares_cols: Iterable[str],
    market_filter: Iterable[str] | None = None,
    pad_code: bool = True,
) -> Tuple[List[str], Dict[str, float]]:
    if not csv_path or not os.path.exists(csv_path):
        return [], {}

    df = None
    for enc in ("utf-8", "cp949"):
        try:
            df = pd.read_csv(csv_path, encoding=enc)
            break
        except UnicodeDecodeError:
            continue
    if df is None:
        return [], {}

    code_col = next((c for c in code_cols if c in df.columns), None)
    if code_col is None:
        return [], {}

    # ë§ˆì¼“ í•„í„°(ì˜µì…˜): Market/ì‹œì¥/Exchange ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ í•´ë‹¹ ê°’ë§Œ ì‚¬ìš©
    if market_filter:
        market_col = next(
            (c for c in ("Market", "ì‹œì¥", "Exchange") if c in df.columns), None
        )
        if market_col is not None:
            allowed = {str(x).upper() for x in market_filter}
            df = df[df[market_col].astype(str).str.upper().isin(allowed)]

    df[code_col] = df[code_col].astype(str).str.strip()
    if pad_code:
        df[code_col] = df[code_col].str.zfill(6)
    codes = df[code_col].tolist()

    shares_col = next((c for c in shares_cols if c in df.columns), None)
    shares_map: Dict[str, float] = {}
    if shares_col is not None:
        shares_series = df[shares_col]
        if shares_series.dtype == object:
            shares_series = shares_series.astype(str).str.replace(",", "")
        shares_series = pd.to_numeric(shares_series, errors="coerce").fillna(0)
        shares_map = {
            str(code).zfill(6): float(val)
            for code, val in zip(df[code_col], shares_series)
        }

    return codes, shares_map


def _normalize_fdr_df(
    df: pd.DataFrame, code: str, shares: float | None = None
) -> pd.DataFrame:
    if df.empty:
        return df
    df = df.reset_index()
    df["Amount"] = df["Close"] * df["Volume"]
    if shares is None or pd.isna(shares):
        shares = 0
    df["Marcap"] = df["Close"] * shares
    df["Code"] = str(code).zfill(6)

    cols = [
        "Date",
        "Code",
        "Open",
        "High",
        "Low",
        "Close",
        "Volume",
        "Amount",
        "Marcap",
    ]
    for c in cols:
        if c not in df.columns:
            df[c] = 0
    return df[cols]


# def get_ohlcv(
#     code: str,
#     start: str | None = None,
#     end: str | None = None,
#     *,
#     preprocess: bool = True,
#     use_marcap: bool = True,
#     use_fdr: bool = True,
#     marcap_dir: str | None = None,
# ) -> pd.DataFrame:
#     """ì¢…ëª© ì½”ë“œì™€ ê¸°ê°„ì„ ë°›ì•„ OHLCV DataFrameì„ ë°˜í™˜í•©ë‹ˆë‹¤.

#     - ê¸°ë³¸: Marcap ê¸°ë°˜(ê°€ëŠ¥ ì‹œ) + FDRë¡œ ìµœì‹  êµ¬ê°„ ì˜µì…˜ ë³´ì™„
#     - ë°˜í™˜ í¬ë§·: preprocess=Trueì´ë©´ date index + ì†Œë¬¸ì ì»¬ëŸ¼(open/high/low/close/volume...)
#     """

#     # end ê¸°ë³¸ê°’ì€ í˜¸ì¶œ ì‹œì  ê¸°ì¤€ (ë¬¸ìì—´)
#     if end is None:
#         end = datetime.now().strftime("%Y-%m-%d")

#     downloader = StockDownloader(
#         base_dir="data/raw",
#         start_date=start or "2005-01-03",
#         end_date=end,
#         preprocess=preprocess,
#         overwrite=False,
#         max_workers=1,
#         marcap_dir=marcap_dir,
#         use_marcap=use_marcap,
#         use_fdr=use_fdr,
#         preload_marcap=False,
#     )
#     return downloader.fetch_ohlcv(code, shares_map={})


def _load_csv(symbol: str, start: str | None, end: str | None) -> pd.DataFrame:
    candidates = [
        Path("data/processed") / f"{symbol}.csv",
        Path("data/raw") / f"{symbol}.csv",
    ]
    path = next((p for p in candidates if p.exists()), None)
    if path is None:
        raise FileNotFoundError(f"CSV not found for {symbol}")

    df = pd.read_csv(path)
    if start or end:
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"], errors="coerce")
            df = df.set_index("date")
        if not isinstance(df.index, pd.DatetimeIndex):
            df.index = pd.to_datetime(df.index, errors="coerce")
        if start:
            df = df.loc[df.index >= pd.to_datetime(start)]
        if end:
            df = df.loc[df.index <= pd.to_datetime(end)]
    return df


def _load_fdr(symbol: str, start: str | None, end: str | None) -> pd.DataFrame:
    try:
        import FinanceDataReader as fdr
    except ImportError as exc:  # pragma: no cover
        raise ImportError(
            "FinanceDataReaderê°€ í•„ìš”í•©ë‹ˆë‹¤. `pip install finance-datareader` ì‹¤í–‰ í›„ ì¬ì‹œë„í•˜ì„¸ìš”."
        ) from exc
    return fdr.DataReader(symbol, start, end)


def load_ohlcv_bulk(
    universe: list[str],
    start: Optional[str] = None,
    end: Optional[str] = None,
    source: str = "fdr",
    use_cache: bool = True,
    cache_dir: str = "data/raw",
    preprocess: bool = True,
) -> Dict[str, pd.DataFrame]:
    """ìœ ë‹ˆë²„ìŠ¤ OHLCV ë¡œë“œ(ìºì‹œ/ì „ì²˜ë¦¬ í¬í•¨).

    - source: "fdr" | "csv"
    - ë°˜í™˜: {symbol: DataFrame}
    """

    data: Dict[str, pd.DataFrame] = {}
    for symbol in universe:
        cache_key = f"{source}:{symbol}:{start}:{end}"  # start/endê¹Œì§€ í¬í•¨í•´ ì•ˆì „ ìºì‹œ
        if use_cache:
            cached = read_cache(cache_key, cache_dir=cache_dir)
            if cached is not None:
                data[symbol] = cached
                continue

        if source == "csv":
            df = _load_csv(symbol, start, end)
        else:
            df = _load_fdr(symbol, start, end)

        if preprocess:
            df = preprocess_ohlcv(df)

        data[symbol] = df
        if use_cache:
            write_cache(cache_key, df, cache_dir=cache_dir)

    return data


class StockDownloader:
    def __init__(
        self,
        base_dir: str = "data/raw",
        start_date: str = "2005-01-03",
        end_date: str = "2026-01-15",
        *,
        preprocess: bool = True,
        overwrite: bool = False,
        max_workers: int = 8,
        marcap_dir: str | None = None,
        # use_marcap: bool = True,
        use_fdr: bool = False,
        preload_marcap: bool = True,
    ):
        """KRX ì „ì²´ ì¢…ëª©ì„ (Marcap + FDR)ë¡œ ë‚´ë ¤ë°›ì•„ íŒŒì¼ë¡œ ì €ì¥.

        ì €ì¥ íŒŒì¼ì€ ì¢…ëª©ë³„ Parquet(ê¸°ë³¸)ì´ë©°, ì €ì¥ ì „ì— preprocess_ohlcvë¡œ ì»¬ëŸ¼/ë‚ ì§œ/ê²°ì¸¡ì„ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - ê¸°ë³¸: Marcap ê¸°ë°˜
        - FDRì€ ì˜µì…˜(use_fdr=True)ì¼ ë•Œë§Œ ìµœì‹  êµ¬ê°„ì„ ì´ì–´ë¶™ì…ë‹ˆë‹¤.
        - preload_marcap=Trueì´ë©´ run()ì—ì„œ Marcap ë°ì´í„°ë¥¼ 1íšŒ ë¡œë”©í•´ ìŠ¤ë ˆë“œì— ê³µìœ í•©ë‹ˆë‹¤.
        """

        self.base_dir = base_dir
        self.start_date = start_date
        self.end_date = end_date
        self.preprocess = preprocess
        self.overwrite = overwrite

        self.max_workers = int(max_workers)
        # self.use_marcap = bool(use_marcap)
        self.use_fdr = bool(use_fdr)
        self.preload_marcap = bool(preload_marcap)

        default_marcap_dir = os.path.join(os.path.dirname(self.base_dir), "marcap_repo")
        self.marcap_dir = marcap_dir or default_marcap_dir  # marcap ì €ì¥ì†Œ ê²½ë¡œ

        self.start_ts = pd.to_datetime(self.start_date)
        self.end_ts = pd.to_datetime(self.end_date)

        self._fdr = None

        os.makedirs(self.base_dir, exist_ok=True)
        # if self.use_marcap:
        #     self._prepare_marcap()

    # def _marcap_parquet_path(self, year: int) -> Path:
    #     return Path(self.marcap_dir) / "data" / f"marcap-{int(year)}.parquet"

    # def get_master_from_marcap(self, *, year: int | None = None) -> pd.DataFrame:
    #     """marcap parquetì—ì„œ Code-Name ë§¤í•‘(ë§ˆìŠ¤í„°)ì„ ìƒì„±í•©ë‹ˆë‹¤."""

    #     if not self.use_marcap:
    #         return pd.DataFrame()

    #     target_year = (
    #         int(year) if year is not None else int(min(self.end_ts.year, 2025))
    #     )
    #     p = self._marcap_parquet_path(target_year)
    #     if not p.exists():
    #         data_dir = Path(self.marcap_dir) / "data"
    #         candidates = sorted(data_dir.glob("marcap-*.parquet"))
    #         if not candidates:
    #             return pd.DataFrame()
    #         p = candidates[-1]

    #     cols = ["Code", "Name", "Market", "Stocks", "Date"]
    #     df = pd.read_parquet(p, columns=cols)
    #     if df.empty or "Code" not in df.columns:
    #         return pd.DataFrame()

    #     df["Code"] = df["Code"].astype(str).str.strip().str.zfill(6)
    #     df = df[df["Code"].str.match(r"^\d{6}$", na=False)]
    #     if "Date" in df.columns:
    #         df["Date"] = pd.to_datetime(df["Date"], errors="coerce")
    #         df = df.sort_values("Date")

    #     # ë§ˆì§€ë§‰ ê°’ ê¸°ì¤€ìœ¼ë¡œ ì´ë¦„/ì‹œì¥/ì£¼ì‹ìˆ˜ í™•ë³´
    #     keep_cols = [c for c in ("Code", "Name", "Market", "Stocks") if c in df.columns]
    #     master = (
    #         df.groupby("Code", as_index=False)[keep_cols].last()
    #         if keep_cols
    #         else df[["Code"]].drop_duplicates()
    #     )
    #     if "Stocks" in master.columns:
    #         master["Stocks"] = pd.to_numeric(master["Stocks"], errors="coerce").fillna(
    #             0
    #         )
    #     return master

    # def get_codes_and_shares_from_marcap(
    #     self, *, year: int | None = None
    # ) -> tuple[list[str], dict[str, float]]:
    #     """marcap_repoì˜ parquetì—ì„œ ì¢…ëª©ì½”ë“œ/ìƒì¥ì£¼ì‹ìˆ˜(Stocks)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""

    #     if not self.use_marcap:
    #         return [], {}

    #     master = self.get_master_from_marcap(year=year)
    #     if master.empty:
    #         return [], {}

    #     codes = master["Code"].astype(str).str.zfill(6).tolist()
    #     shares_map: dict[str, float] = {}
    #     if "Stocks" in master.columns:
    #         shares_map = {
    #             str(code).zfill(6): float(stocks)
    #             for code, stocks in zip(master["Code"], master["Stocks"])
    #         }
    #     return codes, shares_map

    # def _prepare_marcap(self):
    #     """Marcap ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìœ¼ë©´ Git Clone í›„ ê²½ë¡œ ì¶”ê°€"""
    #     # ì´ë¯¸ ê²½ë¡œê°€ ìˆìœ¼ë©´ clone ìƒëµ
    #     if not os.path.exists(self.marcap_dir):
    #         git = shutil.which("git")
    #         if not git:
    #             raise RuntimeError(
    #                 "marcap_repoê°€ ì—†ê³  gitë„ ì—†ìŠµë‹ˆë‹¤. git ì„¤ì¹˜ í›„ ì¬ì‹œë„í•˜ê±°ë‚˜ --use-marcap falseë¡œ ì‹¤í–‰í•˜ì„¸ìš”."
    #             )
    #         logger.info("Marcap ì €ì¥ì†Œ í´ë¡  ì¤‘...")
    #         # shell=Falseë¡œ ì•ˆì „ ì‹¤í–‰
    #         import subprocess

    #         try:
    #             subprocess.run(
    #                 [
    #                     git,
    #                     "clone",
    #                     "--depth",
    #                     "1",
    #                     "https://github.com/FinanceData/marcap.git",
    #                     self.marcap_dir,
    #                 ],
    #                 check=True,
    #             )
    #         except Exception as e:
    #             raise RuntimeError(f"Marcap clone ì‹¤íŒ¨: {e}") from e

    #     # ê²½ë¡œ ì£¼ì…
    #     if self.marcap_dir not in sys.path:
    #         sys.path.append(self.marcap_dir)

    # def _get_marcap_module(self):
    #     import importlib

    #     try:
    #         return importlib.import_module("marcap")
    #     except Exception as exc:
    #         raise ImportError(
    #             "marcap ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (clone ì‹¤íŒ¨/ê²½ë¡œ ì£¼ì… ì‹¤íŒ¨/ì˜ì¡´ì„± ë¬¸ì œ ê°€ëŠ¥)"
    #         ) from exc

    def _get_fdr_module(self):
        if self._fdr is not None:
            return self._fdr
        try:
            import FinanceDataReader as fdr
        except ImportError as exc:
            raise ImportError(
                "FinanceDataReaderê°€ í•„ìš”í•©ë‹ˆë‹¤. `pip install finance-datareader` ì‹¤í–‰ í›„ ì¬ì‹œë„í•˜ì„¸ìš”."
            ) from exc
        self._fdr = fdr
        return fdr

    # def _marcap_for_code(
    #     self,
    #     code: str,
    #     *,
    #     start_ts: pd.Timestamp | None = None,
    #     end_ts: pd.Timestamp | None = None,
    # ) -> pd.DataFrame:
    #     """ì½”ë“œ ë‹¨ìœ„ë¡œ Marcap ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

    #     1) marcap.marcap_dataê°€ ì½”ë“œ í•„í„° ì¸ìë¥¼ ì§€ì›í•˜ë©´ ì½”ë“œ ë‹¨ìœ„ë¡œ ë¡œë“œ(ë©”ëª¨ë¦¬ ì•ˆì „)
    #     2) ì§€ì›í•˜ì§€ ì•Šìœ¼ë©´ ì—°ë„ë³„ë¡œ ì²­í¬ ë¡œë“œ í›„ codeë§Œ í•„í„°ë§(í´ë°±)
    #     """

    #     if not self.use_marcap:
    #         return pd.DataFrame()

    #     code6 = str(code).zfill(6)

    #     eff_start = start_ts or self.start_ts
    #     eff_end = end_ts or self.end_ts

    #     start_year = int(pd.Timestamp(eff_start).year)
    #     end_year = int(min(pd.Timestamp(eff_end).year, 2025))
    #     chunks: list[pd.DataFrame] = []

    #     for y in range(start_year, end_year + 1):
    #         p = self._marcap_parquet_path(y)
    #         if not p.exists():
    #             continue
    #         try:
    #             df_y = pd.read_parquet(
    #                 p,
    #                 columns=[
    #                     "Date",
    #                     "Code",
    #                     "Open",
    #                     "High",
    #                     "Low",
    #                     "Close",
    #                     "Volume",
    #                     "Amount",
    #                     "Marcap",
    #                 ],
    #             )
    #         except Exception:
    #             continue
    #         if df_y.empty or "Code" not in df_y.columns:
    #             continue
    #         df_y["Code"] = df_y["Code"].astype(str).str.strip().str.zfill(6)
    #         df_y = df_y[df_y["Code"] == code6]
    #         if df_y.empty:
    #             continue
    #         chunks.append(df_y)

    #     if not chunks:
    #         return pd.DataFrame()

    #     df = pd.concat(chunks, ignore_index=True)
    #     if "Date" in df.columns:
    #         df["Date"] = pd.to_datetime(df["Date"], errors="coerce")
    #         df = df[(df["Date"] >= eff_start) & (df["Date"] <= eff_end)]
    #     return df

    # def _preload_marcap_groups(
    #     self, target_codes: list[str]
    # ) -> dict[str, pd.DataFrame]:
    #     """Marcap ë°ì´í„°ë¥¼ 1íšŒ ë¡œë”©í•´ ì½”ë“œë³„ DataFrame dictë¡œ êµ¬ì„±.

    #     ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì»¤ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ preload_marcap=Trueì¼ ë•Œë§Œ ì‚¬ìš©.
    #     """

    #     if not self.use_marcap:
    #         return {}

    #     codes = {str(c).zfill(6) for c in target_codes}

    #     start_year = int(self.start_ts.year)
    #     end_year = int(min(self.end_ts.year, 2025))

    #     groups: dict[str, list[pd.DataFrame]] = {}
    #     for y in range(start_year, end_year + 1):
    #         p = self._marcap_parquet_path(y)
    #         if not p.exists():
    #             continue
    #         try:
    #             df_y = pd.read_parquet(
    #                 p,
    #                 columns=[
    #                     "Date",
    #                     "Code",
    #                     "Open",
    #                     "High",
    #                     "Low",
    #                     "Close",
    #                     "Volume",
    #                     "Amount",
    #                     "Marcap",
    #                 ],
    #             )
    #         except Exception:
    #             continue
    #         if df_y.empty or "Code" not in df_y.columns:
    #             continue

    #         df_y["Code"] = df_y["Code"].astype(str).str.strip().str.zfill(6)
    #         df_y = df_y[df_y["Code"].isin(codes)]
    #         if df_y.empty:
    #             continue

    #         for code, df_code in df_y.groupby("Code"):
    #             groups.setdefault(str(code), []).append(df_code)

    #     merged: dict[str, pd.DataFrame] = {}
    #     for code, parts in groups.items():
    #         merged[code] = pd.concat(parts, ignore_index=True)

    #     return merged

    def _get_target_list(self):
        """KRX ì¢…ëª©ì½”ë“œ/ìƒì¥ì£¼ì‹ìˆ˜ í™•ë³´ (ê¸°ë³¸: marcap, ì˜µì…˜: FDR)."""

        # if self.use_marcap:
        #     master = self.get_master_from_marcap()
        #     if not master.empty and "Code" in master.columns:
        #         codes = master["Code"].astype(str).str.zfill(6).tolist()
        #         shares_map: dict[str, float] = {}
        #         if "Stocks" in master.columns:
        #             shares_map = {
        #                 str(code).zfill(6): float(stocks)
        #                 for code, stocks in zip(master["Code"], master["Stocks"])
        #             }

        #         # ì½”ë“œ-ì´ë¦„ ë§¤í•‘ ì €ì¥(StockDownloader ë‹¨ë… ì‹¤í–‰ ì‹œì—ë„ ë‚¨ê¸°ê¸°)
        #         try:
        #             list_path = os.path.join(
        #                 self.base_dir,
        #                 f"krx_master_{datetime.now().strftime('%Y%m%d')}.csv",
        #             )
        #             cols = [
        #                 c
        #                 for c in ("Code", "Name", "Market", "Stocks")
        #                 if c in master.columns
        #             ]
        #             master[cols].to_csv(list_path, index=False, encoding="utf-8-sig")
        #         except Exception:
        #             pass

        #         logger.info(f"KRX ì¢…ëª©ì½”ë“œ í™•ë³´ ì™„ë£Œ (marcap, {len(codes)}ê°œ)")
        #         return codes, shares_map

        if self.use_fdr:
            fdr = self._get_fdr_module()
            stocks = fdr.StockListing("KRX")
            stocks["Code"] = stocks["Code"].astype(str).str.zfill(6)
            stock_col = next(
                (c for c in stocks.columns if "Stocks" in c or "ìƒì¥ì£¼ì‹ìˆ˜" in c), None
            )
            shares_map = (
                stocks.set_index("Code")[stock_col].to_dict() if stock_col else {}
            )
            try:
                list_path = os.path.join(
                    self.base_dir, f"krx_master_{datetime.now().strftime('%Y%m%d')}.csv"
                )
                cols = [
                    c
                    for c in ("Code", "Name", "Market", "Stocks")
                    if c in stocks.columns
                ]
                if cols:
                    stocks[cols].to_csv(list_path, index=False, encoding="utf-8-sig")
            except Exception:
                pass
            logger.info(f"KRX ì¢…ëª©ì½”ë“œ í™•ë³´ ì™„ë£Œ (FDR, {len(stocks)}ê°œ)")
            return stocks["Code"].tolist(), shares_map

        return [], {}

    def process_stock(
        self, code, shares_map, marcap_groups: dict[str, pd.DataFrame] | None = None
    ):
        """ê°œë³„ ì¢…ëª© ì²˜ë¦¬ ë¡œì§ (Marcap + FDR ë³‘í•©)"""
        save_path = os.path.join(self.base_dir, f"{code}.parquet")

        if (not self.overwrite) and os.path.exists(save_path):
            return "Skip"

        try:
            df = self.fetch_ohlcv(
                code, shares_map=shares_map, marcap_groups=marcap_groups
            )
            if df.empty:
                return "Empty"

            # ì €ì¥ í¬ë§·: date ì»¬ëŸ¼ í¬í•¨
            if isinstance(df.index, pd.DatetimeIndex):
                df_to_save = df.reset_index()
            else:
                df_to_save = df
            df_to_save.to_parquet(save_path, index=False)
            return "Success"
        except Exception as e:
            return f"Error: {str(e)}"

    def fetch_ohlcv(
        self,
        code: str,
        *,
        shares_map: Dict[str, float] | None = None,
        # marcap_groups: dict[str, pd.DataFrame] | None = None,
        start_ts: pd.Timestamp | None = None,
        end_ts: pd.Timestamp | None = None,
    ) -> pd.DataFrame:
        """ë‹¨ì¼ ì¢…ëª©ì˜ (Marcap + FDR) OHLCVë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜."""

        shares_map = shares_map or {}
        eff_start = start_ts or self.start_ts
        eff_end = end_ts or self.end_ts

        # # (A) Marcap ë°ì´í„°(ê°€ëŠ¥í•˜ë©´ ì½”ë“œ ë‹¨ìœ„ë¡œ ë¡œë“œ)
        # if marcap_groups is not None:
        #     df_m = marcap_groups.get(str(code).zfill(6), pd.DataFrame())
        # else:
        #     df_m = self._marcap_for_code(code, start_ts=eff_start, end_ts=eff_end)

        # (B) FDR ë°ì´í„°: Marcap ë§ˆì§€ë§‰ ë‚ ì§œ + 1ì¼ë¶€í„° end_dateê¹Œì§€ ìë™ ì—°ê²°
        df_m = pd.DataFrame()
        df_f = pd.DataFrame()
        if self.use_fdr:
            fdr = self._get_fdr_module()

            fdr_start = eff_start
            # if not df_m.empty:
            #     if "Date" in df_m.columns:
            #         last_m = pd.to_datetime(df_m["Date"], errors="coerce").max()
            #     else:
            #         last_m = pd.to_datetime(df_m.index, errors="coerce").max()
            #     if pd.notna(last_m):
            #         fdr_start = pd.Timestamp(last_m) + pd.Timedelta(days=1)

            try:
                df_f = fdr.DataReader(code, fdr_start, eff_end)
            except Exception as exc:
                # FDRì€ ë„¤íŠ¸ì›Œí¬/ì†ŒìŠ¤ ì´ìŠˆê°€ ì¦ìœ¼ë¯€ë¡œ, marcap ë°ì´í„°ë§Œì´ë¼ë„ ì €ì¥ë˜ê²Œ í´ë°±
                # logger.debug(f"FDR ë³´ê°• ì‹¤íŒ¨({code}): {exc}")
                logger.debug(f"FDR ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨({code}): {exc}")
                df_f = pd.DataFrame()

        if not df_f.empty:
            df_f = df_f.reset_index()
            df_f["Amount"] = df_f["Close"] * df_f["Volume"]
            shares = shares_map.get(str(code).zfill(6), shares_map.get(code, 0))
            df_f["Marcap"] = df_f["Close"] * (shares if pd.notna(shares) else 0)
            df_f["Code"] = str(code).zfill(6)

            cols_needed = [
                "Date",
                "Code",
                "Open",
                "High",
                "Low",
                "Close",
                "Volume",
                "Amount",
                "Marcap",
            ]
            for c in cols_needed:
                if c not in df_f.columns:
                    df_f[c] = 0
            df_f = df_f[cols_needed]

        # (C) ë³‘í•©
        cols = [
            "Date",
            "Code",
            "Open",
            "High",
            "Low",
            "Close",
            "Volume",
            "Amount",
            "Marcap",
        ]
        # if not df_m.empty:
        #     for c in cols:
        #         if c not in df_m.columns:
        #             df_m[c] = pd.NA

        #     if not df_f.empty:
        #         last_m_date = pd.to_datetime(df_m["Date"], errors="coerce").max()
        #         if pd.notna(last_m_date):
        #             df_f = df_f[
        #                 pd.to_datetime(df_f["Date"], errors="coerce") > last_m_date
        #             ]
        #         df_final = pd.concat([df_m[cols], df_f], ignore_index=True)
        #     else:
        #         df_final = df_m[cols]
        # else:
        #    df_final = df_f

        df_final = df_f
        
        if df_final.empty:
            return pd.DataFrame()

        df_final = df_final.sort_values("Date").reset_index(drop=True)

        # (D) ì „ì²˜ë¦¬: í‘œì¤€ ì»¬ëŸ¼/ì¸ë±ìŠ¤/ê²°ì¸¡ ì²˜ë¦¬
        if self.preprocess:
            df_final = preprocess_ohlcv(df_final)
        return df_final

    def run(self):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        # 1. íƒ€ê²Ÿ ë¦¬ìŠ¤íŠ¸ í™•ë³´
        target_codes, shares_map = self._get_target_list()

        logger.info(f"ğŸ”¥ ì´ {len(target_codes)}ê°œ ì¢…ëª© ë³€í™˜ ì‹œì‘...")

        # 3. ë£¨í”„ ì‹¤í–‰
        try:
            from tqdm.auto import tqdm
        except Exception:  # pragma: no cover
            tqdm = None  # type: ignore

        def _tqdm(iterable, **kwargs):
            if tqdm is None:
                return iterable
            return tqdm(iterable, **kwargs)

        results = []
        workers = max(1, int(self.max_workers))

        marcap_groups: dict[str, pd.DataFrame] | None = None
        # if self.use_marcap and self.preload_marcap:
        #     logger.info("Marcap ë°ì´í„° ì‚¬ì „ ë¡œë”© ì‹œì‘(ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ ê°€ëŠ¥)")
        #     marcap_groups = self._preload_marcap_groups(target_codes)
        #     logger.info("Marcap ë°ì´í„° ì‚¬ì „ ë¡œë”© ì™„ë£Œ")

        # ë„¤íŠ¸ì›Œí¬ ìš”ì²­(FDR) ë³‘ë ¬í™”: ë„ˆë¬´ í¬ê²Œ ì¡ìœ¼ë©´ ì°¨ë‹¨/ë ˆì´íŠ¸ë¦¬ë°‹ ìœ„í—˜
        with ThreadPoolExecutor(max_workers=workers) as ex:
            futures = {
                ex.submit(self.process_stock, code, shares_map, marcap_groups): code
                for code in target_codes
            }
            for fut in _tqdm(as_completed(futures), total=len(futures)):
                try:
                    results.append(fut.result())
                except Exception as e:
                    results.append(f"Error: {str(e)}")

        logger.info("ì‘ì—… ì™„ë£Œ!")
        logger.info(f"ì„±ê³µ: {results.count('Success')}, ìŠ¤í‚µ: {results.count('Skip')}")
        logger.info(
            f"ë°ì´í„°ì—†ìŒ: {results.count('Empty')}, ì—ëŸ¬: {sum(1 for r in results if r.startswith('Error'))}"
        )


class UnifiedDownloader:
    def __init__(self, config: Dict[str, Any]) -> None:
        cfg = config
        self.data_cfg = cfg.get("data", cfg)
        self.paths_cfg = cfg.get("paths", {})
        self.date_cfg = cfg.get("date", {})
        self.download_cfg = cfg.get("download", {})
        self.market_cfg = cfg.get("markets", {})
        self.lists_cfg = cfg.get("lists", {})
        self.sources_cfg = cfg.get("sources", {})

        self.start_date = self.date_cfg.get("start", "2005-01-03")
        self.end_date = self.date_cfg.get("end") or datetime.now().strftime("%Y-%m-%d")

        self.output_mode = str(self.data_cfg.get("output_mode", "raw")).lower()
        self.preprocess = bool(self.data_cfg.get("preprocess", True))

        self.refresh_master = bool(self.lists_cfg.get("refresh_master", True))

        # self.use_marcap = bool(self.sources_cfg.get("use_marcap", True))
        self.use_fdr = bool(self.sources_cfg.get("use_fdr", True))
        self.preload_marcap = bool(self.sources_cfg.get("preload_marcap", True))

        self._fdr = None

    def _is_kr_kind(self, kind: str) -> bool:
        return kind in ("kr_stocks", "kr_etf")

    def _filter_kr_codes(self, codes: Iterable[str]) -> list[str]:
        # items = [str(c).strip() for c in codes if str(c).strip()]
        items = [t for c in codes if (t := str(c).strip())]
        s = pd.Series([c.zfill(6) for c in items]) # 6 ìë¦¬ ë§ì¶”ê¸°, '0'ìœ¼ë¡œ ì±„ì›€
        valid = s.str.match(r"^\d{6}$", na=False) # ìˆ«ì 6ìë¦¬ì•ˆì§€ í™•ì¸
        return list(dict.fromkeys(s[valid].tolist()))

    def _filter_df_by_codes(
        self,
        df: pd.DataFrame,
        code_col: str,
        codes: Iterable[str],
        *,
        pad_kr: bool = True,
    ) -> pd.DataFrame:
        if df is None or df.empty or code_col not in df.columns:
            return pd.DataFrame()
        base = df.copy()
        s = base[code_col].astype(str).str.strip()
        if pad_kr:
            s = s.str.zfill(6)
            allow = set(self._filter_kr_codes(codes))
        else:
            allow = set(str(c).strip() for c in codes if str(c).strip())
        base[code_col] = s
        return base[base[code_col].isin(allow)]

    def _filter_df_by_valid_kr(self, df: pd.DataFrame, code_col: str) -> pd.DataFrame:
        if df is None or df.empty or code_col not in df.columns:
            return pd.DataFrame()
        s = df[code_col].astype(str).str.strip().str.zfill(6)
        allow = set(self._filter_kr_codes(s.tolist()))
        base = df.copy()
        base[code_col] = s
        return base[base[code_col].isin(allow)]

    def _filter_shares_map(
        self, shares_map: Dict[str, float], allow_codes: Iterable[str]
    ) -> Dict[str, float]:
        allow = set(allow_codes)
        return {
            str(k).zfill(6): float(v)
            for k, v in (shares_map or {}).items()
            if str(k).zfill(6) in allow
        }

    def _collect_local_codes(self, *, kind: str) -> list[str]:
        """ë¡œì»¬ì— ì´ë¯¸ ì €ì¥ëœ parquet íŒŒì¼ëª… ê¸°ì¤€ìœ¼ë¡œ ì½”ë“œ ë¦¬ìŠ¤íŠ¸ ìƒì„±."""

        candidates: list[str] = []
        for processed in (False, True):
            folder = self._resolve_dir(kind, processed=processed)
            try:
                p = Path(folder)
                if not p.exists():
                    continue
                for f in p.glob("*.parquet"):
                    candidates.append(f.stem)
            except Exception:
                continue

        if not candidates:
            return []

        codes = [str(c).strip() for c in candidates if str(c).strip()]
        if self._is_kr_kind(kind):
            codes = self._filter_kr_codes(codes)
        # ìˆœì„œ ìœ ì§€ unique
        return list(dict.fromkeys(codes))

    def _refresh_kr_etf_master_from_local(self) -> None:
        if not self.refresh_master:
            return
        local_codes = self._collect_local_codes(kind="kr_etf")
        if not local_codes:
            logger.warning("ë¡œì»¬ kr_etf parquetì´ ì—†ì–´ ë§ˆìŠ¤í„°ë¥¼ ê°±ì‹ í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        if not self.use_fdr:
            logger.warning(
                "sources.use_fdr=falseë¼ KR ETF ë§ˆìŠ¤í„°(ì´ë¦„/ì¹´í…Œê³ ë¦¬) ê°±ì‹ ì„ ê±´ë„ˆëœë‹ˆë‹¤."
            )
            return

        fdr = self._get_fdr()
        listing = fdr.StockListing("ETF/KR")
        if listing is None or getattr(listing, "empty", True):
            logger.warning("FDR ETF/KR listingì´ ë¹„ì–´ ë§ˆìŠ¤í„°ë¥¼ ê°±ì‹ í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        df = self._filter_df_by_codes(listing, "Symbol", local_codes, pad_kr=True)
        self._save_etf_master(df, us=False)
        logger.info(f"KR ETF ë§ˆìŠ¤í„° ê°±ì‹  ì™„ë£Œ(ë¡œì»¬ ê¸°ì¤€): {len(df)}ê°œ")

    def _refresh_krx_master_from_local(self) -> None:
        if not self.refresh_master:
            return
        local_codes = self._collect_local_codes(kind="kr_stocks")
        if not local_codes:
            logger.warning(
                "ë¡œì»¬ kr_stocks parquetì´ ì—†ì–´ ë§ˆìŠ¤í„°ë¥¼ ê°±ì‹ í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            )
            return

        allow = set(local_codes)

        # # 1) marcap master ìš°ì„  (Code/Name/Market/Stocks)
        # if self.use_marcap:
        #     try:
        #         downloader = StockDownloader(
        #             base_dir=self._resolve_dir("kr_stocks", processed=False),
        #             start_date=self.start_date,
        #             end_date=self.end_date,
        #             preprocess=False,
        #             overwrite=False,
        #             max_workers=1,
        #             use_marcap=True,
        #             use_fdr=False,
        #             preload_marcap=False,
        #         )
        #         master = downloader.get_master_from_marcap()
        #         if master is not None and not master.empty and "Code" in master.columns:
        #             master = self._filter_df_by_codes(
        #                 master, "Code", local_codes, pad_kr=True
        #             )
        #             self._save_krx_master(master)
        #             logger.info(
        #                 f"KR ì£¼ì‹ ë§ˆìŠ¤í„° ê°±ì‹  ì™„ë£Œ(ë¡œì»¬ ê¸°ì¤€, marcap): {len(master)}ê°œ"
        #             )
        #             return
        #     except Exception as exc:
        #         logger.warning(f"marcap ê¸°ë°˜ ë§ˆìŠ¤í„° ê°±ì‹  ì‹¤íŒ¨: {exc}")

        # 2) FDR listing í´ë°± (Code/Name ì •ë„)
        if self.use_fdr:
            try:
                fdr = self._get_fdr()
                stocks = fdr.StockListing(self.market_cfg.get("kr", "KRX"))
                if stocks is None or getattr(stocks, "empty", True):
                    return

                df = stocks.copy()
                if "Code" not in df.columns:
                    return
                df = self._filter_df_by_codes(df, "Code", local_codes, pad_kr=True)
                # save schema normalize
                out = pd.DataFrame({"Code": df["Code"]})
                name_col = (
                    "Name"
                    if "Name" in df.columns
                    else ("íšŒì‚¬ëª…" if "íšŒì‚¬ëª…" in df.columns else None)
                )
                if name_col:
                    out["Name"] = df[name_col]
                self._save_krx_master(out)
                logger.info(f"KR ì£¼ì‹ ë§ˆìŠ¤í„° ê°±ì‹  ì™„ë£Œ(ë¡œì»¬ ê¸°ì¤€, FDR): {len(out)}ê°œ")
            except Exception as exc:
                logger.warning(f"FDR ê¸°ë°˜ ë§ˆìŠ¤í„° ê°±ì‹  ì‹¤íŒ¨: {exc}")

    def _get_fdr(self):
        if self._fdr is not None:
            return self._fdr
        try:
            import FinanceDataReader as fdr
        except ImportError as exc:
            raise ImportError(
                "FinanceDataReaderê°€ í•„ìš”í•©ë‹ˆë‹¤. `pip install finance-datareader` ì‹¤í–‰ í›„ ì¬ì‹œë„í•˜ì„¸ìš”."
            ) from exc
        self._fdr = fdr
        return fdr

    def _resolve_dir(self, kind: str, processed: bool) -> str:
        group = "processed" if processed else "raw"
        path_map = self.paths_cfg.get(group) or {}
        if kind in path_map:
            path = str(path_map[kind])
            if not os.path.isabs(path):
                base_dir = self.data_cfg.get("base_dir", "data")
                return os.path.join(base_dir, path)
            return path
        base_dir = self.data_cfg.get("base_dir", "data")
        folder = "processed" if processed else "raw"
        return os.path.join(base_dir, folder, kind)

    def _save_df(self, df: pd.DataFrame, code: str, *, kind: str) -> None:
        if df.empty:
            return

        # list_only ëª¨ë“œì—ì„œëŠ” parquet ì €ì¥ì„ í•˜ì§€ ì•ŠìŒ
        if self.output_mode == "list_only":
            return

        incremental = bool(self.data_cfg.get("incremental", True))

        # raw ì €ì¥
        if self.output_mode in ("raw", "both"):
            raw_dir = self._resolve_dir(kind, processed=False)
            os.makedirs(raw_dir, exist_ok=True)
            raw_path = os.path.join(raw_dir, f"{str(code).zfill(6)}.parquet")
            df_raw = df.copy()
            if isinstance(df_raw.index, pd.DatetimeIndex):
                df_raw = df_raw.reset_index()

            if incremental and os.path.exists(raw_path):
                try:
                    exist = pd.read_parquet(raw_path)
                    # ë‚ ì§œ ì»¬ëŸ¼ í†µì¼
                    if "date" in exist.columns and "Date" not in exist.columns:
                        exist = exist.rename(columns={"date": "Date"})
                    if "date" in df_raw.columns and "Date" not in df_raw.columns:
                        df_raw = df_raw.rename(columns={"date": "Date"})
                    merged = pd.concat([exist, df_raw], ignore_index=True)
                    if "Date" in merged.columns:
                        merged["Date"] = pd.to_datetime(merged["Date"], errors="coerce")
                        merged = merged.dropna(subset=["Date"]).sort_values("Date")
                        merged = merged.drop_duplicates(subset=["Date"], keep="last")
                    df_raw = merged
                except Exception:
                    pass
            df_raw.to_parquet(raw_path, index=False)

        # processed ì €ì¥
        if self.output_mode in ("processed", "both"):
            processed_dir = self._resolve_dir(kind, processed=True)
            os.makedirs(processed_dir, exist_ok=True)
            processed_path = os.path.join(
                processed_dir, f"{str(code).zfill(6)}.parquet"
            )
            df_proc = preprocess_ohlcv(df) if self.preprocess else df.copy()
            if isinstance(df_proc.index, pd.DatetimeIndex):
                df_proc = df_proc.reset_index()

            if incremental and os.path.exists(processed_path):
                try:
                    exist = pd.read_parquet(processed_path)
                    merged = pd.concat([exist, df_proc], ignore_index=True)
                    if "date" in merged.columns:
                        merged["date"] = pd.to_datetime(merged["date"], errors="coerce")
                        merged = merged.dropna(subset=["date"]).sort_values("date")
                        merged = merged.drop_duplicates(subset=["date"], keep="last")
                    df_proc = merged
                except Exception:
                    pass
            df_proc.to_parquet(processed_path, index=False)

    def _save_krx_master(self, master: pd.DataFrame) -> None:
        if master is None or master.empty:
            return

        cols = [c for c in ("Code", "Name", "Market", "Stocks") if c in master.columns]
        if not cols:
            return

        df = master.copy()
        df["Code"] = df["Code"].astype(str).str.zfill(6)
        self._write_master_csv(df[cols], prefix="krx_master")

    def _write_master_csv(self, payload: pd.DataFrame, *, prefix: str) -> None:
        if payload is None or payload.empty:
            return

        base_dir = self.data_cfg.get("base_dir", "data")
        out_dir = Path(base_dir) / "raw"
        out_dir.mkdir(parents=True, exist_ok=True)

        latest = out_dir / f"{prefix}_latest.csv"
        dated = out_dir / f"{prefix}_{datetime.now().strftime('%Y%m%d')}.csv"

        for path in (latest, dated):
            try:
                payload.to_csv(path, index=False, encoding="utf-8-sig")
            except PermissionError as exc:
                alt = (
                    out_dir
                    / f"{path.stem}_{datetime.now().strftime('%H%M%S')}{path.suffix}"
                )
                logger.warning(
                    f"master CSV ì €ì¥ ì‹¤íŒ¨(ì ê¸ˆ ê°€ëŠ¥): {path} ({exc}). ëŒ€ì²´ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤: {alt}"
                )
                payload.to_csv(alt, index=False, encoding="utf-8-sig")
            except Exception as exc:
                logger.warning(f"master CSV ì €ì¥ ì‹¤íŒ¨: {path} ({exc})")

    def _save_etf_master(self, master: pd.DataFrame, *, us: bool) -> None:
        if master is None or master.empty:
            return
        df = master.copy()
        if "Symbol" in df.columns:
            if not us:
                df = self._filter_df_by_valid_kr(df, "Symbol")
            else:
                df["Symbol"] = df["Symbol"].astype(str).str.strip()

        # ê°€ëŠ¥í•œ ì»¬ëŸ¼ë§Œ ì €ì¥ (FDR ë²„ì „ë³„ ì°¨ì´ ëŒ€ì‘)
        cols = [c for c in ("Symbol", "Name", "Category", "MarCap") if c in df.columns]
        if not cols:
            cols = list(df.columns)
        prefix = f"{'us_etf' if us else 'kr_etf'}_master"
        self._write_master_csv(df[cols], prefix=prefix)

    def _plan_incremental(
        self, *, codes: List[str], kind: str
    ) -> dict[str, str | None]:
        """ì½”ë“œë³„ë¡œ ì–´ëŠ ì‹œì ë¶€í„° ë‹¤ì‹œ ë°›ì„ì§€(start_date override)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        - None: ì „ì²´ ë²”ìœ„ í•„ìš”(íŒŒì¼ ì—†ìŒ)
        - "SKIP": ì´ë¯¸ ë²”ìœ„ ì¶©ì¡±
        - "YYYY-MM-DD": í•´ë‹¹ ë‚ ì§œë¶€í„° ì¦ë¶„ ìˆ˜ì§‘
        """

        incremental = bool(self.data_cfg.get("incremental", True))
        skip_if_covered = bool(self.data_cfg.get("skip_if_covered", True))

        start_ts = pd.to_datetime(self.start_date)
        end_ts = pd.to_datetime(self.end_date)

        # ì–´ë–¤ íŒŒì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ìŠ¤í‚µ íŒë‹¨í• ì§€: raw ìš°ì„ 
        raw_dir = self._resolve_dir(kind, processed=False)
        processed_dir = self._resolve_dir(kind, processed=True)

        plan: dict[str, str | None] = {}
        for code in codes:
            code6 = str(code).zfill(6)
            raw_path = os.path.join(raw_dir, f"{code6}.parquet")
            proc_path = os.path.join(processed_dir, f"{code6}.parquet")

            target_path = raw_path if os.path.exists(raw_path) else proc_path
            if not os.path.exists(target_path):
                plan[code6] = None
                continue

            s, e = _get_parquet_date_range(target_path)
            if s is None or e is None:
                plan[code6] = None
                continue

            if skip_if_covered and s <= start_ts and e >= end_ts:
                plan[code6] = "SKIP"
                continue

            if incremental and e < end_ts:
                inc_start = (pd.Timestamp(e) + pd.Timedelta(days=1)).strftime(
                    "%Y-%m-%d"
                )
                plan[code6] = inc_start
                continue

            plan[code6] = None

        return plan

    def _download_codes(
        self,
        codes: List[str],
        *,
        kind: str,
        fetch_fn,
        shares_map: Dict[str, float] | None = None,
        max_workers: int = 8,
    ) -> None:
        if not codes:
            return
        shares_map = shares_map or {}

        try:
            from tqdm.auto import tqdm
        except Exception:  # pragma: no cover
            tqdm = None  # type: ignore

        def _tqdm(iterable, **kwargs):
            if tqdm is None:
                return iterable
            return tqdm(iterable, **kwargs)

        workers = max(1, int(max_workers))
        with ThreadPoolExecutor(max_workers=workers) as ex:
            futures = {ex.submit(fetch_fn, code, shares_map): code for code in codes}
            for fut in _tqdm(as_completed(futures), total=len(futures)):
                code = futures[fut]
                try:
                    df = fut.result()
                    self._save_df(df, code, kind=kind)
                except Exception as e:
                    logger.warning(f"{kind} {code} ì‹¤íŒ¨: {e}")

    def _get_krx_list(self) -> Tuple[List[str], Dict[str, float]]:
        use_list = bool(self.lists_cfg.get("use_list_stocks", False))
        if use_list:
            csv_path = self.lists_cfg.get("stocks_csv")
            codes, shares = _read_csv_codes(
                csv_path,
                code_cols=("ì¢…ëª©ì½”ë“œ", "Code", "Ticker", "Symbol"),
                shares_cols=("ìƒì¥ì£¼ì‹ìˆ˜", "Stocks", "ìƒì¥ì¢Œìˆ˜", "Shares"),
                market_filter=("KRX", "KOSPI", "KOSDAQ"),
                pad_code=True,
            )
            if codes:
                filtered_codes = self._filter_kr_codes(codes)
                shares = self._filter_shares_map(shares, filtered_codes)
                return filtered_codes, shares

        # # ê¸°ë³¸ ê²½ë¡œ: marcap(ë¡œì»¬ parquet)ë¡œ ì¢…ëª©ì½”ë“œ í™•ë³´ (ë„¤íŠ¸ì›Œí¬ ë¶ˆí•„ìš”)
        # if self.use_marcap:
        #     try:
        #         downloader = StockDownloader(
        #             base_dir=self._resolve_dir("kr_stocks", processed=False),
        #             start_date=self.start_date,
        #             end_date=self.end_date,
        #             preprocess=False,
        #             overwrite=False,
        #             max_workers=1,
        #             use_marcap=True,
        #             use_fdr=False,
        #             preload_marcap=False,
        #         )
        #         master = downloader.get_master_from_marcap()
        #         if self.refresh_master:
        #             self._save_krx_master(master)
        #         codes, shares_map = downloader.get_codes_and_shares_from_marcap()
        #         if codes:
        #             codes = self._filter_kr_codes(codes)
        #             shares_map = self._filter_shares_map(shares_map, codes)
        #             logger.info(f"KRX ì¢…ëª©ì½”ë“œ í™•ë³´ ì™„ë£Œ (marcap, {len(codes)}ê°œ)")
        #             return codes, shares_map
        #     except Exception as exc:
        #         logger.warning(f"marcapì—ì„œ ì¢…ëª©ì½”ë“œ í™•ë³´ ì‹¤íŒ¨: {exc}")

        # ì˜µì…˜: FDR listing (ì›í•˜ë©´ configì— sources.use_fdr_listing: true)
        allow_fdr_listing = bool(self.sources_cfg.get("use_fdr_listing", False))
        if not allow_fdr_listing:
            # ë§ˆì§€ë§‰ í´ë°±: ì €ì¥ëœ CSVê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            fallback_csv = _latest_krx_list_csv(self.data_cfg.get("base_dir", "data"))
            if fallback_csv:
                codes, shares = _read_csv_codes(
                    fallback_csv,
                    code_cols=("ì¢…ëª©ì½”ë“œ", "Code", "Ticker", "Symbol"),
                    shares_cols=("ìƒì¥ì£¼ì‹ìˆ˜", "Stocks", "ìƒì¥ì¢Œìˆ˜", "Shares"),
                    market_filter=("KRX", "KOSPI", "KOSDAQ"),
                    pad_code=True,
                )
                if codes:
                    filtered_codes = self._filter_kr_codes(codes)
                    shares = self._filter_shares_map(shares, filtered_codes)
                    logger.info(f"KRX ì¢…ëª©ì½”ë“œ í™•ë³´ ì™„ë£Œ (ì €ì¥ëœ CSV, {len(codes)}ê°œ)")
                    return filtered_codes, shares

            logger.error(
                "KRX ì¢…ëª©ì½”ë“œ í™•ë³´ ì‹¤íŒ¨: marcap/CSV ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                "(í•„ìš” ì‹œ sources.use_fdr_listing=trueë¡œ FDR listingë„ í—ˆìš©í•˜ì„¸ìš”.)"
            )
            return [], {}

        fdr = self._get_fdr()
        stocks = fdr.StockListing(self.market_cfg.get("kr", "KRX"))
        stocks = self._filter_df_by_valid_kr(stocks, "Code")
        stock_col = next(
            (c for c in stocks.columns if "Stocks" in c or "ìƒì¥ì£¼ì‹ìˆ˜" in c), None
        )
        shares_map = stocks.set_index("Code")[stock_col].to_dict() if stock_col else {}

        # ëª©ë¡ ìë™ ì €ì¥ (ë‹¤ìŒ ì‹¤í–‰ í´ë°±ìš©)
        if self.refresh_master:
            try:
                base_dir = self.data_cfg.get("base_dir", "data")
                list_dir = Path(base_dir) / "raw"
                list_dir.mkdir(parents=True, exist_ok=True)
                list_path = (
                    list_dir / f"krx_codes_{datetime.now().strftime('%Y%m%d')}.csv"
                )
                cols = ["Code"] + (["Name"] if "Name" in stocks.columns else [])
                stocks[cols].to_csv(list_path, index=False, encoding="utf-8-sig")
            except Exception:
                pass

        return stocks["Code"].tolist(), shares_map

    def _get_us_list(self) -> List[str]:
        use_list = bool(self.lists_cfg.get("use_list_stocks", False))
        if use_list:
            csv_path = self.lists_cfg.get("stocks_csv")
            codes, _ = _read_csv_codes(
                csv_path,
                code_cols=("Ticker", "Symbol", "Code"),
                shares_cols=("Shares", "Stocks", "ìƒì¥ì£¼ì‹ìˆ˜"),
                market_filter=("NASDAQ", "NYSE", "AMEX", "US"),
                pad_code=False,
            )
            if codes:
                return codes

        fdr = self._get_fdr()
        markets = self.market_cfg.get("us", ["NASDAQ", "NYSE", "AMEX"])
        tickers: List[str] = []
        for m in markets:
            try:
                listing = fdr.StockListing(m)
            except Exception:
                continue
            col = "Symbol" if "Symbol" in listing.columns else "Code"
            tickers.extend(listing[col].astype(str).tolist())
        return list(dict.fromkeys(tickers))

    def _get_etf_list(self, *, us: bool) -> Tuple[List[str], Dict[str, float]]:
        use_list = bool(self.lists_cfg.get("use_list_etf", True))
        if use_list:
            csv_path = self.lists_cfg.get("etf_csv")
            codes, shares = _read_csv_codes(
                csv_path,
                code_cols=("ì¢…ëª©ì½”ë“œ", "Code", "Ticker", "Symbol"),
                shares_cols=("ìƒì¥ì¢Œìˆ˜", "ìƒì¥ì£¼ì‹ìˆ˜", "Stocks", "Shares"),
                market_filter=(
                    ("NASDAQ", "NYSE", "AMEX", "US")
                    if us
                    else ("KRX", "KOSPI", "KOSDAQ")
                ),
                pad_code=(not us),
            )
            # CSVì— ì´ìƒí•œ ì½”ë“œ(ì•ŒíŒŒë‰´ë©”ë¦­)ê°€ ì„ì—¬ ìˆì„ ìˆ˜ ìˆì–´ KRì€ 6ìë¦¬ ìˆ«ìë§Œ í†µê³¼
            if not us and codes:
                filtered_codes = self._filter_kr_codes(codes)
                shares = self._filter_shares_map(shares, filtered_codes)
                return filtered_codes, shares
            return codes, shares

        # ë¦¬ìŠ¤íŠ¸ë¥¼ ì“°ì§€ ì•ŠëŠ” ê²½ìš°: FDRì˜ ETF listingì„ ì‚¬ìš©
        if not self.use_fdr:
            logger.error(
                "ETF ëª©ë¡ í™•ë³´ ì‹¤íŒ¨: lists.use_list_etf=false ì´ê³  sources.use_fdr=false ì…ë‹ˆë‹¤."
            )
            return [], {}

        market = "ETF/US" if us else "ETF/KR"
        try:
            fdr = self._get_fdr()
            listing = fdr.StockListing(market)
            if listing is None or getattr(listing, "empty", True):
                logger.error(
                    f"ETF ëª©ë¡ í™•ë³´ ì‹¤íŒ¨: FDR StockListing('{market}') ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
                )
                return [], {}

            code_col = (
                "Symbol"
                if "Symbol" in listing.columns
                else ("Code" if "Code" in listing.columns else None)
            )
            if code_col is None:
                logger.error(
                    f"ETF ëª©ë¡ í™•ë³´ ì‹¤íŒ¨: FDR listing ì»¬ëŸ¼ì— Symbol/Codeê°€ ì—†ìŠµë‹ˆë‹¤. columns={list(listing.columns)}"
                )
                return [], {}

            df = listing.copy()
            df[code_col] = df[code_col].astype(str).str.strip()

            if not us:
                # KR ETFëŠ” KRX 6ìë¦¬ ìˆ«ì ì½”ë“œë§Œ ì‚¬ìš© (ì•ŒíŒŒë‰´ë©”ë¦­ ì‹¬ë³¼ì€ ì œì™¸)
                df = self._filter_df_by_valid_kr(df, code_col)

            codes = df[code_col].astype(str).tolist()
            codes = list(dict.fromkeys([c for c in codes if c]))

            if self.refresh_master:
                self._save_etf_master(df, us=us)
            logger.info(f"ETF ëª©ë¡ í™•ë³´ ì™„ë£Œ (FDR {market}, {len(codes)}ê°œ)")
            return codes, {}
        except Exception as exc:
            logger.error(f"ETF ëª©ë¡ í™•ë³´ ì‹¤íŒ¨ (FDR {market}): {exc}")
            return [], {}

    # def _fetch_kr(self, code: str, shares_map: Dict[str, float]) -> pd.DataFrame:
    #     downloader = StockDownloader(
    #         base_dir=self._resolve_dir("kr_stocks", processed=False),
    #         start_date=self.start_date,
    #         end_date=self.end_date,
    #         preprocess=False,
    #         overwrite=False,
    #         max_workers=1,
    #         use_marcap=self.use_marcap,
    #         use_fdr=self.use_fdr,
    #         preload_marcap=self.preload_marcap,
    #     )
    #     return downloader.fetch_ohlcv(code, shares_map=shares_map)

    def _fetch_us(self, code: str, _: Dict[str, float]) -> pd.DataFrame:
        fdr = self._get_fdr()
        df = fdr.DataReader(code, self.start_date, self.end_date)
        return _normalize_fdr_df(df, code, shares=None)

    def _fetch_kr_etf(self, code: str, shares_map: Dict[str, float]) -> pd.DataFrame:
        fdr = self._get_fdr()
        df = fdr.DataReader(code, self.start_date, self.end_date)
        shares = shares_map.get(str(code).zfill(6), shares_map.get(code, 0))
        return _normalize_fdr_df(df, code, shares=shares)

    def _fetch_us_etf(self, code: str, _: Dict[str, float]) -> pd.DataFrame:
        fdr = self._get_fdr()
        df = fdr.DataReader(code, self.start_date, self.end_date)
        return _normalize_fdr_df(df, code, shares=None)

    def run(self, *, max_workers: int = 8) -> None:
        # output_mode=list_only: ë¡œì»¬ parquetì— ì¡´ì¬í•˜ëŠ” ì½”ë“œë§Œ ê¸°ì¤€ìœ¼ë¡œ ë§ˆìŠ¤í„° CSV ë®ì–´ì“°ê¸°
        if self.output_mode == "list_only":
            self._refresh_krx_master_from_local()
            self._refresh_kr_etf_master_from_local()
            logger.info("output_mode=list_only: ë‹¤ìš´ë¡œë“œëŠ” ìˆ˜í–‰í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        if self.download_cfg.get("kr_stocks", False):
            codes, shares = self._get_krx_list()
            if codes:
                plan = self._plan_incremental(
                    codes=[str(c).zfill(6) for c in codes], kind="kr_stocks"
                )
                codes_to_run = [c for c in codes if plan.get(str(c).zfill(6)) != "SKIP"]

                kr_downloader = StockDownloader(
                    base_dir=self._resolve_dir("kr_stocks", processed=False),
                    start_date=self.start_date,
                    end_date=self.end_date,
                    preprocess=False,
                    overwrite=False,
                    max_workers=1,
                    # use_marcap=self.use_marcap,
                    use_fdr=self.use_fdr,
                    preload_marcap=self.preload_marcap,
                )

                # marcap_groups: dict[str, pd.DataFrame] | None = None
                # if self.use_marcap and self.preload_marcap:
                #     logger.info("Marcap ë°ì´í„° ì‚¬ì „ ë¡œë”© ì‹œì‘(ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ ê°€ëŠ¥)")
                #     marcap_groups = kr_downloader._preload_marcap_groups(
                #         [str(c).zfill(6) for c in codes_to_run]
                #     )
                #     logger.info("Marcap ë°ì´í„° ì‚¬ì „ ë¡œë”© ì™„ë£Œ")

                def _fetch_kr_shared(
                    code: str, shares_map: Dict[str, float]
                ) -> pd.DataFrame:
                    code6 = str(code).zfill(6)
                    start_override = plan.get(code6)
                    if start_override and start_override != "SKIP":
                        return kr_downloader.fetch_ohlcv(
                            code6,
                            shares_map=shares_map,
                            # marcap_groups=marcap_groups,
                            start_ts=pd.to_datetime(start_override),
                            end_ts=pd.to_datetime(self.end_date),
                        )
                    return kr_downloader.fetch_ohlcv(
                        code,
                        shares_map=shares_map,
                        # marcap_groups=marcap_groups,
                    )

                self._download_codes(
                    codes_to_run,
                    kind="kr_stocks",
                    fetch_fn=_fetch_kr_shared,
                    shares_map=shares,
                    max_workers=max_workers,
                )

        if self.download_cfg.get("us_stocks", False):
            codes = self._get_us_list()
            self._download_codes(
                codes,
                kind="us_stocks",
                fetch_fn=self._fetch_us,
                shares_map={},
                max_workers=max_workers,
            )

        if self.download_cfg.get("kr_etf", False):
            codes, shares = self._get_etf_list(us=False)
            if not codes:
                logger.warning(
                    "kr_etf ë‹¤ìš´ë¡œë“œê°€ í™œì„±í™”ëì§€ë§Œ ETF ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. "
                    "(lists.use_list_etf=falseë©´ FDR ETF/KR listingì´ í•„ìš”í•©ë‹ˆë‹¤.)"
                )
            self._download_codes(
                codes,
                kind="kr_etf",
                fetch_fn=self._fetch_kr_etf,
                shares_map=shares,
                max_workers=max_workers,
            )

        if self.download_cfg.get("us_etf", False):
            codes, shares = self._get_etf_list(us=True)
            if not codes:
                logger.warning(
                    "us_etf ë‹¤ìš´ë¡œë“œê°€ í™œì„±í™”ëì§€ë§Œ ETF ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. "
                    "(lists.use_list_etf=falseë©´ FDR ETF/US listingì´ í•„ìš”í•©ë‹ˆë‹¤.)"
                )
            self._download_codes(
                codes,
                kind="us_etf",
                fetch_fn=self._fetch_us_etf,
                shares_map=shares,
                max_workers=max_workers,
            )


if __name__ == "__main__":
    import argparse

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s",
    )

    parser = argparse.ArgumentParser(description="Unified data downloader")
    parser.add_argument(
        "--config",
        type=str,
        default="data",
        help="config/{name}.yaml",
    )
    parser.add_argument("--max-workers", type=int, default=8)
    args = parser.parse_args()

    cfg = load_data_config(args.config)
    ud = UnifiedDownloader(cfg)
    logger.info(
        f"base_dir={ud.data_cfg.get('base_dir')}, output_mode={ud.data_cfg.get('output_mode')}"
    )
    logger.info(f"download={ud.download_cfg}")
    ud.run(max_workers=int(args.max_workers))
